{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11332596,"sourceType":"datasetVersion","datasetId":7089121},{"sourceId":11426132,"sourceType":"datasetVersion","datasetId":7156190},{"sourceId":11492515,"sourceType":"datasetVersion","datasetId":7204237},{"sourceId":11553970,"sourceType":"datasetVersion","datasetId":7244947},{"sourceId":11588813,"sourceType":"datasetVersion","datasetId":7266621},{"sourceId":11620026,"sourceType":"datasetVersion","datasetId":7289667},{"sourceId":11629059,"sourceType":"datasetVersion","datasetId":7296063},{"sourceId":11629123,"sourceType":"datasetVersion","datasetId":7296099},{"sourceId":11629813,"sourceType":"datasetVersion","datasetId":7296594},{"sourceId":11631109,"sourceType":"datasetVersion","datasetId":7297513},{"sourceId":11632935,"sourceType":"datasetVersion","datasetId":7298684},{"sourceId":11635137,"sourceType":"datasetVersion","datasetId":7300188},{"sourceId":11635496,"sourceType":"datasetVersion","datasetId":7300462}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Attempt to get this working in spring 2025: \n# https://github.com/magenta/magenta/blob/main/magenta/models/sketch_rnn/sketch_rnn_train.py\n\n!python --version\n# We're expecting Python 3.10.12","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:40:59.031224Z","iopub.execute_input":"2025-05-01T07:40:59.031544Z","iopub.status.idle":"2025-05-01T07:40:59.154540Z","shell.execute_reply.started":"2025-05-01T07:40:59.031506Z","shell.execute_reply":"2025-05-01T07:40:59.153599Z"}},"outputs":[{"name":"stdout","text":"Python 3.10.12\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport sys\nimport contextlib\n\n# Suppress pip warnings and dependency conflict errors from showing in notebook\nwith contextlib.redirect_stdout(open(os.devnull, 'w')), contextlib.redirect_stderr(open(os.devnull, 'w')):\n    !pip install --quiet tensorflow==2.13.0 numpy==1.24.3 six requests tensorflow-probability==0.12.2\n\n# This installs:\n# - TensorFlow 2.13.0 (needed for Sketch-RNN)\n# - NumPy 1.24.3 (before `np.bool` breaks compatibility)\n# - TensorFlow Probability 0.12.2 (clean and stable with TF 2.13)\n#\n# We're intentionally ignoring dependency warnings from unrelated Kaggle packages like\n# `google-api-core`, `tensorflow-text`, and `tf-keras`. These aren't used and the conflict is benign.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:40:59.155574Z","iopub.execute_input":"2025-05-01T07:40:59.155896Z","iopub.status.idle":"2025-05-01T07:41:02.920012Z","shell.execute_reply.started":"2025-05-01T07:40:59.155871Z","shell.execute_reply":"2025-05-01T07:41:02.918819Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Clean up any previous copy of the Magenta repo (in case of reruns)\n!rm -rf magenta\n\n# Clone the official Magenta repository (shallow clone)\n!git clone --depth 1 https://github.com/magenta/magenta.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:41:04.677745Z","iopub.execute_input":"2025-05-01T07:41:04.678066Z","iopub.status.idle":"2025-05-01T07:41:07.105399Z","shell.execute_reply.started":"2025-05-01T07:41:04.678042Z","shell.execute_reply":"2025-05-01T07:41:07.104477Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'magenta'...\nremote: Enumerating objects: 531, done.\u001b[K\nremote: Counting objects: 100% (531/531), done.\u001b[K\nremote: Compressing objects: 100% (481/481), done.\u001b[K\nremote: Total 531 (delta 95), reused 213 (delta 39), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (531/531), 18.92 MiB | 19.37 MiB/s, done.\nResolving deltas: 100% (95/95), done.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Patch np.bool to avoid deprecation error in TensorFlow Probability\nimport numpy as np\nnp.bool = bool  # For compatibility with older TFP code\n\n# Set TensorFlow Probability backend before importing TFP\nimport os\nos.environ['TFP_BACKEND'] = 'tensorflow'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:41:08.448298Z","iopub.execute_input":"2025-05-01T07:41:08.448648Z","iopub.status.idle":"2025-05-01T07:41:08.453264Z","shell.execute_reply.started":"2025-05-01T07:41:08.448620Z","shell.execute_reply":"2025-05-01T07:41:08.452194Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Add Magenta to Python path so we can import its modules\nimport sys\nsys.path.append('/kaggle/working/magenta')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:41:09.925956Z","iopub.execute_input":"2025-05-01T07:41:09.926266Z","iopub.status.idle":"2025-05-01T07:41:09.930116Z","shell.execute_reply.started":"2025-05-01T07:41:09.926243Z","shell.execute_reply":"2025-05-01T07:41:09.929236Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Install `note_seq`, a dependency of Magenta's drum pipelines\n!pip install --quiet note_seq\n\n# You might see some pip warnings about version conflicts with unrelated packages.\n# That's expected and harmless â€” Sketch-RNN will still work just fine.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:41:11.081869Z","iopub.execute_input":"2025-05-01T07:41:11.082175Z","iopub.status.idle":"2025-05-01T07:41:14.820379Z","shell.execute_reply.started":"2025-05-01T07:41:11.082153Z","shell.execute_reply":"2025-05-01T07:41:14.819116Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Import Sketch-RNN model and utilities from Magenta\n# run twice, error will show on first run\nfrom magenta.models.sketch_rnn import model as sketch_rnn_model\nfrom magenta.models.sketch_rnn import utils","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:41:16.045019Z","iopub.execute_input":"2025-05-01T07:41:16.045331Z","iopub.status.idle":"2025-05-01T07:41:20.586055Z","shell.execute_reply.started":"2025-05-01T07:41:16.045307Z","shell.execute_reply":"2025-05-01T07:41:20.585316Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/internal/backend/numpy/dtype.py:112: FutureWarning: In the future `np.str` will be defined as the corresponding NumPy scalar.\n  string = getattr(np, 'str', getattr(np, 'string', None))\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Cell: Verify installed versions and key imports\n\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nimport numpy as np\nimport importlib.util\n\nimport tensorflow as tf\nprint(\"TensorFlow version:\", tf.__version__)\nprint(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n\n\n\nprint(\"TensorFlow version:           \", tf.__version__)\nprint(\"TensorFlow v1 compatibility:  \", hasattr(tf, 'compat') and hasattr(tf.compat, 'v1'))\nprint(\"TensorFlow Probability:       \", tfp.__version__)\nprint(\"NumPy version:                \", np.__version__)\nprint(\"np.bool exists:               \", hasattr(np, 'bool'))\n\ndef check_module(path):\n    return importlib.util.find_spec(path) is not None\n\nprint(\"Magenta available:            \", check_module(\"magenta\"))\nprint(\"note_seq available:           \", check_module(\"note_seq\"))\n\ntry:\n    from magenta.models.sketch_rnn import model as sketch_rnn_model\n    from magenta.models.sketch_rnn import utils as sketch_rnn_utils\n    print(\"Sketch-RNN model import:      âœ… Success\")\nexcept Exception as e:\n    print(\"Sketch-RNN model import:      âŒ Failed\")\n    print(e)\n\n\n# You should see: \n# \n# TensorFlow version:            2.13.0\n# TensorFlow v1 compatibility:   True\n# TensorFlow Probability:        0.12.2\n# NumPy version:                 1.26.4\n# np.bool exists:                True\n# Magenta available:             True\n# note_seq available:            True\n# Sketch-RNN model import:      âœ… Success","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:41:23.605931Z","iopub.execute_input":"2025-05-01T07:41:23.606529Z","iopub.status.idle":"2025-05-01T07:41:23.701905Z","shell.execute_reply.started":"2025-05-01T07:41:23.606497Z","shell.execute_reply":"2025-05-01T07:41:23.700909Z"}},"outputs":[{"name":"stdout","text":"TensorFlow version: 2.13.0\nGPU Available: []\nTensorFlow version:            2.13.0\nTensorFlow v1 compatibility:   True\nTensorFlow Probability:        0.12.2\nNumPy version:                 1.24.3\nnp.bool exists:                True\nMagenta available:             True\nnote_seq available:            True\nSketch-RNN model import:      âœ… Success\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Imports and setup for Sketch-RNN training\n\nimport json\nimport os\nimport time\nimport zipfile\n\nfrom six.moves.urllib.request import urlretrieve\n\n# Use TensorFlow v1 compatibility mode\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\n\n# Everything else (NumPy, Magenta, utils, etc.) is already handled above.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:41:26.314966Z","iopub.execute_input":"2025-05-01T07:41:26.315267Z","iopub.status.idle":"2025-05-01T07:41:26.319573Z","shell.execute_reply.started":"2025-05-01T07:41:26.315246Z","shell.execute_reply":"2025-05-01T07:41:26.318765Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"#download David haa's kanji dataset \n!mkdir -p /kaggle/working/kanji\n!wget https://github.com/hardmaru/sketch-rnn-datasets/raw/master/kanji/kanji.rdp25.npz -O /kaggle/working/kanji/kanji.rdp25.npz\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\ndef hex_filename_to_char(filename):\n    hex_code = os.path.splitext(filename)[0]  # strip \".svg\"\n    return chr(int(hex_code, 16))\n\n# Directory with your SVG files\nsvg_dir = '/kaggle/working/kanji'  # or wherever you've stored them\n\n# List all characters youâ€™re including\nsvg_files = sorted(os.listdir(svg_dir))  # make sure itâ€™s sorted to preserve order\nchar_list = [hex_filename_to_char(f) for f in svg_files if f.endswith('.svg')]\n\n# Save the list as JSON for use later\nimport json\nwith open('kanji_index.json', 'w', encoding='utf-8') as f:\n    json.dump(char_list, f, ensure_ascii=False, indent=2)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stroke_data = convert_svg_folder_to_stroke3('/kaggle/input/handwritten-english-words/eng')\nnp.savez_compressed(\n    'english,npz',\n    train=np.array(stroke_data[:800], dtype=object),\n    valid=np.array(stroke_data[800:900], dtype=object),\n    test=np.array(stroke_data[900:1000], dtype=object)\n)\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = np.load('english,npz.npz', allow_pickle=True)\n\nprint(\"Keys:\", list(data.keys()))\nprint(\"Train samples:\", len(data['train']))\nprint(\"Valid samples:\", len(data['valid']))\nprint(\"Test samples:\", len(data['test']))","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"english = np.load('/kaggle/working/english,npz.npz', allow_pickle=True, encoding='latin1')\nkanji = np.load('/kaggle/working/kanji/kanji.rdp25.npz', allow_pickle=True, encoding='latin1')\n\n# Sample & balance each set\ntrain = list(english['train'][:800]) + list(kanji['train'][:1000])\nvalid = list(english['valid'][:100]) + list(kanji['valid'][:100])\ntest  = list(english['test'][:100])  + list(kanji['test'][:100])\n\nimport random\nrandom.shuffle(train)\nrandom.shuffle(valid)\nrandom.shuffle(test)\n\nnp.savez_compressed('eng_kanji_joint.npz',\n    train=np.array(train, dtype=object),\n    valid=np.array(valid, dtype=object),\n    test=np.array(test, dtype=object)\n)\n\njoint = np.load('eng_kanji_joint.npz', allow_pickle=True)\nprint(len(joint['train']), len(joint['valid']), len(joint['test']))","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#check if the new dataset is valid\nsample_stroke = joint['test'][0]\nprint(\"Stroke sample (first 5 points):\", sample_stroke[:5])\nprint(\"Shape:\", sample_stroke.shape)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Setup: logging and hyperparameter placeholders\n\n# Use TF1 logging (prints INFO messages)\ntf.logging.set_verbosity(tf.logging.INFO)\n\n# Define configuration options (used to be tf.app.flags)\nclass Config:\n    data_dir = '/kaggle/working/moon_dataset.npz'\n    #'https://github.com/hardmaru/sketch-rnn-datasets/raw/master/aaron_sheep'\n    log_root = '/kaggle/input/kanji-95'\n    #'/tmp/sketch_rnn/models/default'\n    resume_training = False\n    hparams = ''  # Can be populated with comma-separated key=value pairs\n\nFLAGS = Config()\n\n# URL for optional pre-trained models (used later)\nPRETRAINED_MODELS_URL = (\n    'http://download.magenta.tensorflow.org/models/sketch_rnn.zip'\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:41:31.396747Z","iopub.execute_input":"2025-05-01T07:41:31.397157Z","iopub.status.idle":"2025-05-01T07:41:31.401940Z","shell.execute_reply.started":"2025-05-01T07:41:31.397127Z","shell.execute_reply":"2025-05-01T07:41:31.400891Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Utility function to reset the TensorFlow v1 graph and session\ndef reset_graph():\n    \"\"\"Closes the current default session and resets the graph (TF v1).\"\"\"\n    sess = tf.get_default_session()\n    if sess:\n        sess.close()\n    tf.reset_default_graph()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:41:33.450122Z","iopub.execute_input":"2025-05-01T07:41:33.450478Z","iopub.status.idle":"2025-05-01T07:41:33.454721Z","shell.execute_reply.started":"2025-05-01T07:41:33.450446Z","shell.execute_reply":"2025-05-01T07:41:33.453713Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Loads model hyperparameters for inference mode (used after training)\ndef load_model(model_dir):\n    \"\"\"Loads model for inference mode, used in sampling/drawing.\"\"\"\n    model_params = sketch_rnn_model.get_default_hparams()\n    \n    # Load config from model_dir (must contain model_config.json)\n    with tf.gfile.Open(os.path.join(model_dir, 'model_config.json'), 'r') as f:\n        model_params.parse_json(f.read())\n\n    # Optional: override config inline if FLAGS.hparams is set\n    if FLAGS.hparams:\n        model_params.parse(FLAGS.hparams)\n\n    # In inference mode, we only sample one at a time\n    model_params.batch_size = 1\n\n    # Disable dropout for inference\n    eval_model_params = sketch_rnn_model.copy_hparams(model_params)\n    eval_model_params.use_input_dropout = 0\n    eval_model_params.use_recurrent_dropout = 0\n    eval_model_params.use_output_dropout = 0\n    eval_model_params.is_training = 0\n\n    # Use step-by-step sampling\n    sample_model_params = sketch_rnn_model.copy_hparams(eval_model_params)\n    sample_model_params.max_seq_len = 1\n\n    return [model_params, eval_model_params, sample_model_params]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:41:33.998079Z","iopub.execute_input":"2025-05-01T07:41:33.998371Z","iopub.status.idle":"2025-05-01T07:41:34.003815Z","shell.execute_reply.started":"2025-05-01T07:41:33.998351Z","shell.execute_reply":"2025-05-01T07:41:34.002827Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Loads the model hyperparameters and dataset for inference (used in notebooks)\ndef load_env(data_dir, model_dir):\n    \"\"\"Loads environment for inference mode, used in Jupyter notebook.\"\"\"\n    model_params = sketch_rnn_model.get_default_hparams()\n    with tf.gfile.Open(os.path.join(model_dir, 'model_config.json'), 'r') as f:\n        model_params.parse_json(f.read())\n    return load_dataset(data_dir, model_params, inference_mode=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:41:34.546896Z","iopub.execute_input":"2025-05-01T07:41:34.547200Z","iopub.status.idle":"2025-05-01T07:41:34.551789Z","shell.execute_reply.started":"2025-05-01T07:41:34.547176Z","shell.execute_reply":"2025-05-01T07:41:34.550920Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Downloads and extracts pretrained Sketch-RNN models from the Magenta site\ndef download_pretrained_models(\n    models_root_dir='/tmp/sketch_rnn/models',\n    pretrained_models_url=PRETRAINED_MODELS_URL):\n    \"\"\"Download pretrained models to a temporary directory.\"\"\"\n    tf.gfile.MakeDirs(models_root_dir)\n\n    # Full path to downloaded zip file\n    zip_path = os.path.join(models_root_dir, os.path.basename(pretrained_models_url))\n\n    if os.path.isfile(zip_path):\n        tf.logging.info('%s already exists, using cached copy', zip_path)\n    else:\n        tf.logging.info('Downloading pretrained models from %s...', pretrained_models_url)\n        urlretrieve(pretrained_models_url, zip_path)\n        tf.logging.info('Download complete.')\n\n    # Unzip model files\n    tf.logging.info('Unzipping %s...', zip_path)\n    with zipfile.ZipFile(zip_path) as models_zip:\n        models_zip.extractall(models_root_dir)\n    tf.logging.info('Unzipping complete.')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:41:36.652208Z","iopub.execute_input":"2025-05-01T07:41:36.652556Z","iopub.status.idle":"2025-05-01T07:41:36.659217Z","shell.execute_reply.started":"2025-05-01T07:41:36.652525Z","shell.execute_reply":"2025-05-01T07:41:36.657811Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Check if 'utils' from Sketch-RNN is loaded and has key functions/classes\nprint(\"utils module path:       \", utils.__file__)\nprint(\"Has get_max_len:         \", hasattr(utils, \"get_max_len\"))\nprint(\"Has DataLoader class:    \", hasattr(utils, \"DataLoader\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:41:36.973028Z","iopub.execute_input":"2025-05-01T07:41:36.973370Z","iopub.status.idle":"2025-05-01T07:41:36.978264Z","shell.execute_reply.started":"2025-05-01T07:41:36.973345Z","shell.execute_reply":"2025-05-01T07:41:36.977331Z"}},"outputs":[{"name":"stdout","text":"utils module path:        /kaggle/working/magenta/magenta/models/sketch_rnn/utils.py\nHas get_max_len:          True\nHas DataLoader class:     True\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Loads stroke dataset (.npz), splits into train/valid/test, normalizes, and builds data loaders\ndef load_dataset(data_dir, model_params, inference_mode=False):\n    \"\"\"Loads the .npz file, and splits the set into train/valid/test.\"\"\"\n\n    # Support for multiple datasets (if list), or a single dataset\n    datasets = model_params.data_set if isinstance(model_params.data_set, list) else [model_params.data_set]\n\n    train_strokes, valid_strokes, test_strokes = None, None, None\n\n    for dataset in datasets:\n        if data_dir.startswith('http://') or data_dir.startswith('https://'):\n            data_filepath = f'{data_dir}/{dataset}'\n            tf.logging.info('Downloading %s', data_filepath)\n            response = requests.get(data_filepath)\n            # Explicitly allow pickle when loading from URL\n            data = np.load(six.BytesIO(response.content), encoding='latin1', allow_pickle=True)\n        else:\n            data_filepath = os.path.join(data_dir, dataset)\n            # Already had allow_pickle=True here\n            data = np.load(data_filepath, encoding='latin1', allow_pickle=True)\n\n        tf.logging.info('Loaded {}/{}/{} from {}'.format(\n            len(data['train']), len(data['valid']), len(data['test']), dataset))\n\n        if train_strokes is None:\n            train_strokes = data['train']\n            valid_strokes = data['valid']\n            test_strokes = data['test']\n        else:\n            train_strokes = np.concatenate((train_strokes, data['train']))\n            valid_strokes = np.concatenate((valid_strokes, data['valid']))\n            test_strokes = np.concatenate((test_strokes, data['test']))\n\n    # Print dataset stats\n    all_strokes = np.concatenate((train_strokes, valid_strokes, test_strokes))\n    num_points = sum(len(s) for s in all_strokes)\n    avg_len = num_points / len(all_strokes)\n    tf.logging.info('Dataset combined: {} ({}/{}/{}), avg len {}'.format(\n        len(all_strokes), len(train_strokes), len(valid_strokes),\n        len(test_strokes), int(avg_len)))\n\n    # Compute max sequence length\n    max_seq_len = utils.get_max_len(all_strokes)\n    model_params.max_seq_len = max_seq_len\n    tf.logging.info('model_params.max_seq_len %i.', max_seq_len)\n\n    # Build alternate hparam sets\n    eval_model_params = sketch_rnn_model.copy_hparams(model_params)\n    eval_model_params.use_input_dropout = 0\n    eval_model_params.use_recurrent_dropout = 0\n    eval_model_params.use_output_dropout = 0\n    eval_model_params.is_training = 1\n\n    if inference_mode:\n        eval_model_params.batch_size = 1\n        eval_model_params.is_training = 0\n\n    sample_model_params = sketch_rnn_model.copy_hparams(eval_model_params)\n    sample_model_params.batch_size = 1\n    sample_model_params.max_seq_len = 1\n\n    # Construct training loader\n    train_set = utils.DataLoader(\n        train_strokes,\n        model_params.batch_size,\n        max_seq_length=model_params.max_seq_len,\n        random_scale_factor=model_params.random_scale_factor,\n        augment_stroke_prob=model_params.augment_stroke_prob)\n\n    normalizing_scale_factor = train_set.calculate_normalizing_scale_factor()\n    train_set.normalize(normalizing_scale_factor)\n\n    # Construct validation/test loaders (no augmentation)\n    valid_set = utils.DataLoader(\n        valid_strokes,\n        eval_model_params.batch_size,\n        max_seq_length=eval_model_params.max_seq_len,\n        random_scale_factor=0.0,\n        augment_stroke_prob=0.0)\n    valid_set.normalize(normalizing_scale_factor)\n\n    test_set = utils.DataLoader(\n        test_strokes,\n        eval_model_params.batch_size,\n        max_seq_length=eval_model_params.max_seq_len,\n        random_scale_factor=0.0,\n        augment_stroke_prob=0.0)\n    test_set.normalize(normalizing_scale_factor)\n\n    tf.logging.info('normalizing_scale_factor %4.4f.', normalizing_scale_factor)\n\n    return [\n        train_set, valid_set, test_set,\n        model_params, eval_model_params, sample_model_params\n    ]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:41:37.275179Z","iopub.execute_input":"2025-05-01T07:41:37.275498Z","iopub.status.idle":"2025-05-01T07:41:37.285973Z","shell.execute_reply.started":"2025-05-01T07:41:37.275475Z","shell.execute_reply":"2025-05-01T07:41:37.285011Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Evaluate the model on a given dataset\n\ndef evaluate_model(sess, model, data_set):\n    if data_set.num_batches == 0:\n        print(\"[WARNING] Skipping evaluation: validation set too small.\")\n        return 0.0, 0.0, 0.0  # dummy values to keep training going\n\n    \"\"\"Returns the average weighted cost, reconstruction cost and KL cost.\"\"\"\n    total_cost = 0.0\n    total_r_cost = 0.0\n    total_kl_cost = 0.0\n\n    # Loop over all batches in the dataset\n    for batch in range(data_set.num_batches):\n        unused_orig_x, x, s = data_set.get_batch(batch)\n        feed = {\n            model.input_data: x,\n            model.sequence_lengths: s\n        }\n\n        # Run session to compute loss values\n        cost, r_cost, kl_cost = sess.run(\n            [model.cost, model.r_cost, model.kl_cost], feed)\n\n        total_cost += cost\n        total_r_cost += r_cost\n        total_kl_cost += kl_cost\n\n    # Return average over all batches\n    total_cost /= data_set.num_batches\n    total_r_cost /= data_set.num_batches\n    total_kl_cost /= data_set.num_batches\n    return total_cost, total_r_cost, total_kl_cost","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:41:42.340458Z","iopub.execute_input":"2025-05-01T07:41:42.340792Z","iopub.status.idle":"2025-05-01T07:41:42.346272Z","shell.execute_reply.started":"2025-05-01T07:41:42.340767Z","shell.execute_reply":"2025-05-01T07:41:42.345225Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Load model weights from a checkpoint\ndef load_checkpoint(sess, checkpoint_path):\n  \"\"\"Loads the latest checkpoint into the given session.\"\"\"\n  saver = tf.train.Saver(tf.global_variables())\n  ckpt = tf.train.get_checkpoint_state(checkpoint_path)\n\n  tf.logging.info('Loading model %s.', ckpt.model_checkpoint_path)\n\n  # Restore variables from checkpoint\n  saver.restore(sess, ckpt.model_checkpoint_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:41:42.672207Z","iopub.execute_input":"2025-05-01T07:41:42.672514Z","iopub.status.idle":"2025-05-01T07:41:42.677199Z","shell.execute_reply.started":"2025-05-01T07:41:42.672490Z","shell.execute_reply":"2025-05-01T07:41:42.676288Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Save model checkpoint to a given path\ndef save_model(sess, model_save_path, global_step):\n  \"\"\"Saves model variables to checkpoint files.\"\"\"\n  saver = tf.train.Saver(tf.global_variables())\n  checkpoint_path = os.path.join(model_save_path, 'vector')\n\n  tf.logging.info('Saving model to %s.', checkpoint_path)\n  tf.logging.info('Global step: %i.', global_step)\n  tf.gfile.MakeDirs(model_save_path)\n\n  saver.save(sess, checkpoint_path, global_step=global_step)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:41:44.384019Z","iopub.execute_input":"2025-05-01T07:41:44.384337Z","iopub.status.idle":"2025-05-01T07:41:44.388971Z","shell.execute_reply.started":"2025-05-01T07:41:44.384313Z","shell.execute_reply":"2025-05-01T07:41:44.388006Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def train(sess, model, eval_model, train_set, valid_set, test_set):\n  \"\"\"Train a sketch-rnn model.\"\"\"\n\n  # Calculate trainable params.\n  t_vars = tf.trainable_variables()\n  count_t_vars = sum(np.prod(var.get_shape().as_list()) for var in t_vars)\n  print(f\"Total trainable variables: {count_t_vars}\")\n\n  # Setup eval stats\n  best_valid_cost = float('inf')\n  valid_cost = 0.0\n\n  hps = model.hps\n  start = time.time()\n\n  for _ in range(hps.num_steps):\n    step = sess.run(model.global_step)\n\n    curr_learning_rate = ((hps.learning_rate - hps.min_learning_rate) *\n                          (hps.decay_rate)**step + hps.min_learning_rate)\n    curr_kl_weight = (hps.kl_weight - (hps.kl_weight - hps.kl_weight_start) *\n                      (hps.kl_decay_rate)**step)\n\n    _, x, s = train_set.random_batch()\n    feed = {\n        model.input_data: x,\n        model.sequence_lengths: s,\n        model.lr: curr_learning_rate,\n        model.kl_weight: curr_kl_weight\n    }\n\n    (train_cost, r_cost, kl_cost, _, train_step, _) = sess.run([\n        model.cost, model.r_cost, model.kl_cost, model.final_state,\n        model.global_step, model.train_op\n    ], feed)\n\n    if step % 20 == 0 and step > 0:\n      end = time.time()\n      print(f\"Step {step}: cost={train_cost:.4f}, recon={r_cost:.4f}, kl={kl_cost:.4f}, time={end - start:.2f}s\")\n      start = time.time()\n\n    if step % hps.save_every == 0 and step > 0:\n      valid_cost, valid_r_cost, valid_kl_cost = evaluate_model(sess, eval_model, valid_set)\n      print(f\"[VALID] Step {step}: cost={valid_cost:.4f}, recon={valid_r_cost:.4f}, kl={valid_kl_cost:.4f}\")\n\n    try:\n        save_model(sess, FLAGS.log_root, step)\n        print(f\"[INFO] Saved model at step {step}\")\n    except Exception as e:\n        print(f\"[ERROR] Failed to save model at step {step}: {e}\")\n\n        if valid_cost < best_valid_cost:\n            best_valid_cost = valid_cost\n\n        eval_cost, eval_r_cost, eval_kl_cost = evaluate_model(sess, eval_model, test_set)\n        print(f\"[EVAL] Step {step}: cost={eval_cost:.4f}, recon={eval_r_cost:.4f}, kl={eval_kl_cost:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:41:44.715491Z","iopub.execute_input":"2025-05-01T07:41:44.715830Z","iopub.status.idle":"2025-05-01T07:41:44.723723Z","shell.execute_reply.started":"2025-05-01T07:41:44.715806Z","shell.execute_reply":"2025-05-01T07:41:44.722675Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def trainer(model_params, train_set=None, valid_set=None, test_set=None):\n  \"\"\"Top-level training function to prepare data, models, and launch training.\"\"\"\n\n  # Format NumPy output for readability\n  np.set_printoptions(precision=8, edgeitems=6, linewidth=200, suppress=True)\n\n  tf.logging.info('Sketch-RNN Trainer Initialized.')\n  tf.logging.info('Loading dataset...')\n\n  # Ensure FLAGS.hparams (like dataset_name) are parsed into model_params\n  if FLAGS.hparams:\n      model_params.parse(FLAGS.hparams)\n\n  # Load training, validation, and test sets + updated hyperparams\n  datasets = load_dataset(FLAGS.data_dir, model_params)\n  train_set, valid_set, test_set = datasets[:3]\n  model_params, eval_model_params = datasets[3:5]\n\n  # Reset TensorFlow graph\n  reset_graph()\n\n  # Create training and evaluation models\n  model = sketch_rnn_model.Model(model_params)\n  eval_model = sketch_rnn_model.Model(eval_model_params, reuse=True)\n\n  # Start interactive TF session and initialize variables\n  sess = tf.InteractiveSession()\n  sess.run(tf.global_variables_initializer())\n\n  # Optionally resume from checkpoint\n  if FLAGS.resume_training:\n    load_checkpoint(sess, FLAGS.log_root)\n\n  # Save current model config to JSON\n  tf.gfile.MakeDirs(FLAGS.log_root)\n  with tf.gfile.Open(os.path.join(FLAGS.log_root, 'model_config.json'), 'w') as f:\n    f.write(model_params.to_json())\n\n\n  # Begin training loop\n  train(sess, model, eval_model, train_set, valid_set, test_set)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:41:46.556615Z","iopub.execute_input":"2025-05-01T07:41:46.556969Z","iopub.status.idle":"2025-05-01T07:41:46.563039Z","shell.execute_reply.started":"2025-05-01T07:41:46.556943Z","shell.execute_reply":"2025-05-01T07:41:46.562061Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def main(unused_argv):\n  \"\"\"Main entry point: parses hyperparams and launches training.\"\"\"\n  # Load default model hyperparameters\n  model_params = sketch_rnn_model.get_default_hparams()\n\n  # Optionally override with user-defined hparams\n  if FLAGS.hparams:\n    model_params.parse(FLAGS.hparams)\n  # Load datasets and possibly updated model_params\n  datasets = load_dataset(FLAGS.data_dir, model_params, inference_mode=False)\n    \n  train_set, valid_set, test_set = datasets[:3]\n  model_params, eval_model_params = datasets[3:5]\n    \n  # Re-parse hparams AFTER dataset in case max_seq_len/etc overwrote them\n  if FLAGS.hparams:\n    model_params.parse(FLAGS.hparams)\n\n  # Start the training loop\n  trainer(model_params, train_set, valid_set, test_set)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:41:46.960458Z","iopub.execute_input":"2025-05-01T07:41:46.960812Z","iopub.status.idle":"2025-05-01T07:41:46.965843Z","shell.execute_reply.started":"2025-05-01T07:41:46.960784Z","shell.execute_reply":"2025-05-01T07:41:46.965038Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"#if resuming previous training from saved checkpoints uploaded to kaggle/input\n#transfer dataset from /Kaggle/input - read only - to /Kaggle/working\n\nimport shutil\nimport os\n\nlog_dir = '/kaggle/working/sketch_rnn_log_kanji'\n\n# Remove the folder and all its contents\nshutil.rmtree(log_dir, ignore_errors=True) \n\nnew_folder = \"/kaggle/working/sketch_rnn_log_kanji\"\nos.makedirs(new_folder, exist_ok=True)\n\n# # Copy the checkpoint and config from input folder to working\n# if not os.path.exists('/kaggle/working/sketch_rnn_log_kanji'):\n#     shutil.copytree('/kaggle/input/kanji-95', '/kaggle/working/sketch_rnn_log_kanji')\n\n# # Now set log_root to the writable folder\n# FLAGS.log_root = '/kaggle/working/sketch_rnn_log_kanji'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T08:40:07.355364Z","iopub.execute_input":"2025-05-01T08:40:07.355725Z","iopub.status.idle":"2025-05-01T08:40:07.368005Z","shell.execute_reply.started":"2025-05-01T08:40:07.355699Z","shell.execute_reply":"2025-05-01T08:40:07.367130Z"}},"outputs":[],"execution_count":193},{"cell_type":"code","source":"# -------------------------------------------------------------\n# OVERRIDE DEFAULT FLAGS FOR TRAINING\n# -------------------------------------------------------------\n# We're bypassing tf.app.flags (which is designed for command-line use)\n# by directly setting the global FLAGS object in the notebook.\n#\n# This is necessary because:\n# - Notebooks don't use command-line arguments\n# - tf.app.flags would otherwise use empty/default values\n# \n# These overrides are specific to this first test run.\n# You can change them later to try different datasets or hyperparameters.\n\nFLAGS.data_dir = '/kaggle/working'\n# Location of the .npz training dataset (can be remote URL or local path)\n\nFLAGS.log_root = '/kaggle/working/sketch_rnn_log_kanji'\n# Where to save training logs, checkpoints, and model configs\n\nFLAGS.hparams = 'data_set=[emma_skt.npz],num_steps=3001,save_every=500'\n\n# Override default hyperparameters â€” here we're doing a short test run:\n# - 1000 training steps total\n# - Save model and evaluate every 200 steps\n\nFLAGS.resume_training = False\n# Start from scratch. Set to True to continue from saved checkpoint later.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:41:54.309990Z","iopub.execute_input":"2025-05-01T07:41:54.310290Z","iopub.status.idle":"2025-05-01T07:41:54.314487Z","shell.execute_reply.started":"2025-05-01T07:41:54.310268Z","shell.execute_reply":"2025-05-01T07:41:54.313622Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Needed for fetching datasets from the web\nimport requests\n\n# Needed for cross-version compatibility (e.g., BytesIO)\nimport six","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:48:09.850087Z","iopub.execute_input":"2025-05-01T07:48:09.850442Z","iopub.status.idle":"2025-05-01T07:48:09.854240Z","shell.execute_reply.started":"2025-05-01T07:48:09.850416Z","shell.execute_reply":"2025-05-01T07:48:09.853379Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"#Pre-training checklist \n\n#check dataset file exists \nimport os\n\ndataset_path = '/kaggle/working/emma_skt.npz'\nprint(\"Found dataset:\", os.path.exists(dataset_path))\n\n#Validate dataset contents \nimport numpy as np\n\ndata = np.load(dataset_path, allow_pickle=True)\nprint(\"Keys in dataset:\", list(data.keys()))\nprint(\"Train samples:\", len(data['train']))\nprint(\"Valid samples:\", len(data['valid']))\nprint(\"Test samples:\", len(data['test']))\n\n#Check Hyperparams Are Set\nprint(\"Current hparams:\", FLAGS.hparams)\nassert 'data_set=[emma_skt.npz]' in FLAGS.hparams, \"Missing data_set in hparams\"\n\n#Check Log Directory\nprint(\"Log directory:\", FLAGS.log_root)\nos.makedirs(FLAGS.log_root, exist_ok=True)\n\n#dry-run dataset l\nmodel_params = sketch_rnn_model.get_default_hparams()\nmodel_params.parse(FLAGS.hparams)\n\n# Will also set max_seq_len, etc.\n_ = load_dataset(FLAGS.data_dir, model_params, inference_mode=False)\nprint(\"Dataset load: âœ…\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:48:11.980784Z","iopub.execute_input":"2025-05-01T07:48:11.981095Z","iopub.status.idle":"2025-05-01T07:48:12.034293Z","shell.execute_reply.started":"2025-05-01T07:48:11.981074Z","shell.execute_reply":"2025-05-01T07:48:12.033315Z"}},"outputs":[{"name":"stdout","text":"Found dataset: True\nKeys in dataset: ['train', 'valid', 'test']\nTrain samples: 173\nValid samples: 22\nTest samples: 22\nCurrent hparams: data_set=[emma_skt.npz],num_steps=3001,save_every=500\nLog directory: /kaggle/working/sketch_rnn_log_kanji\ntotal images <= max_seq_len is 173\ntotal images <= max_seq_len is 22\ntotal images <= max_seq_len is 22\nDataset load: âœ…\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# -------------------------------------------------------------\n# RUN TRAINING (manually, no tf.app.run())\n# -------------------------------------------------------------\n# - This will initialize the model, load data, and train it\n# - It will save logs and checkpoints to FLAGS.log_root\n\nimport sys\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\n\n# Clear any junk flags added by Jupyter\nsys.argv = ['main.py']\n\n# Use absl.flags\nfrom absl import flags\nFLAGS = flags.FLAGS\n\n# Define flags only if not already defined\nif 'data_dir' not in FLAGS:\n    flags.DEFINE_string('data_dir', '', '')\n    flags.DEFINE_string('log_root', '', '')\n    flags.DEFINE_boolean('resume_training', False, '')\n    flags.DEFINE_string('hparams', '', '')\n\n# Parse the flags (must do before accessing them)\nFLAGS(sys.argv)\n\n# Set values manually\nFLAGS.data_dir = '/kaggle/working/'\nFLAGS.log_root = '/kaggle/working/sketch_rnn_log_kanji'\nFLAGS.resume_training = False\nFLAGS.hparams = 'data_set=[emma_skt.npz],num_steps=10001,save_every=500'\n\n# Now safe to access\nprint(\"Log root is:\", FLAGS.log_root)\n\n# Run the training loop directly (no tf.app.run)\nmain(None)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nlog_dir = FLAGS.log_root\nprint(\"Files in log directory:\")\nprint(os.listdir(log_dir))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#zip model \n!zip -r /kaggle/working/emma_model.zip /kaggle/working/sketch_rnn_log_kanji","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#=========== using the trained model ============\\\n!pip install -qU svgwrite","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:48:18.209547Z","iopub.execute_input":"2025-05-01T07:48:18.209915Z","iopub.status.idle":"2025-05-01T07:48:22.022523Z","shell.execute_reply.started":"2025-05-01T07:48:18.209888Z","shell.execute_reply":"2025-05-01T07:48:22.021427Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"import svgwrite\nfrom IPython.display import SVG, display\nimport PIL\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport logging\ntf.logging.info(\"TensorFlow Version: %s\", tf.__version__)\n\nnp.set_printoptions(precision=8, edgeitems=6, linewidth=200, suppress=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:48:22.024148Z","iopub.execute_input":"2025-05-01T07:48:22.024508Z","iopub.status.idle":"2025-05-01T07:48:22.065390Z","shell.execute_reply.started":"2025-05-01T07:48:22.024470Z","shell.execute_reply":"2025-05-01T07:48:22.064531Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"\nfrom magenta.models.sketch_rnn.model import *\nfrom magenta.models.sketch_rnn.utils import *\nfrom magenta.models.sketch_rnn.rnn import *","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:48:23.402905Z","iopub.execute_input":"2025-05-01T07:48:23.403240Z","iopub.status.idle":"2025-05-01T07:48:23.407408Z","shell.execute_reply.started":"2025-05-01T07:48:23.403210Z","shell.execute_reply":"2025-05-01T07:48:23.406388Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"def draw_strokes(data, factor=0.2, svg_filename='/tmp/sketch_rnn/svg/sample.svg'):\n    tf.io.gfile.makedirs(os.path.dirname(svg_filename))\n    min_x, max_x, min_y, max_y = get_bounds(data, factor)\n    dims = (50 + max_x - min_x, 50 + max_y - min_y)\n    dwg = svgwrite.Drawing(svg_filename, size=dims)\n    dwg.add(dwg.rect(insert=(0, 0), size=dims, fill='white'))\n    lift_pen = 1\n    abs_x = 25 - min_x \n    abs_y = 25 - min_y\n    p = \"M%s,%s \" % (abs_x, abs_y)\n    command = \"m\"\n    for i in range(len(data)):\n        if lift_pen == 1:\n            command = \"m\"\n        elif command != \"l\":\n            command = \"l\"\n        else:\n            command = \"\"\n        x = float(data[i, 0]) / factor\n        y = float(data[i, 1]) / factor\n        lift_pen = data[i, 2]\n        p += command + str(x) + \",\" + str(y) + \" \"\n    dwg.add(dwg.path(p).stroke(\"black\", 1).fill(\"none\"))\n    dwg.save()\n    display(SVG(dwg.tostring()))\n\ndef make_grid_svg(s_list, grid_space=10.0, grid_space_x=16.0):\n    def get_start_and_end(x):\n        x = np.array(x)\n        x = x[:, 0:2]\n        x_start = x[0]\n        x_end = x.sum(axis=0)\n        x = x.cumsum(axis=0)\n        x_max = x.max(axis=0)\n        x_min = x.min(axis=0)\n        center_loc = (x_max + x_min) * 0.5\n        return x_start - center_loc, x_end\n\n    x_pos = 0.0\n    y_pos = 0.0\n    result = [[x_pos, y_pos, 1]]\n    for sample in s_list:\n        s = sample[0]\n        grid_loc = sample[1]\n        grid_y = grid_loc[0] * grid_space + grid_space * 0.5\n        grid_x = grid_loc[1] * grid_space_x + grid_space_x * 0.5\n        start_loc, delta_pos = get_start_and_end(s)\n        loc_x = start_loc[0]\n        loc_y = start_loc[1]\n        new_x_pos = grid_x + loc_x\n        new_y_pos = grid_y + loc_y\n        result.append([new_x_pos - x_pos, new_y_pos - y_pos, 0])\n        result += s.tolist()\n        result[-1][2] = 1\n        x_pos = new_x_pos + delta_pos[0]\n        y_pos = new_y_pos + delta_pos[1]\n    return np.array(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:48:24.097283Z","iopub.execute_input":"2025-05-01T07:48:24.097629Z","iopub.status.idle":"2025-05-01T07:48:24.108013Z","shell.execute_reply.started":"2025-05-01T07:48:24.097604Z","shell.execute_reply":"2025-05-01T07:48:24.106930Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"!mkdir -p /kaggle/working/kanji\n!wget https://github.com/hardmaru/sketch-rnn-datasets/raw/master/kanji/kanji.rdp25.npz -O /kaggle/working/kanji/kanji.rdp25.npz","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cp -r /kaggle/input/emma-sketches-model/emma_skt/* /kaggle/working/sketch_rnn_log_kanji/\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T08:43:28.640406Z","iopub.execute_input":"2025-05-01T08:43:28.640803Z","iopub.status.idle":"2025-05-01T08:43:28.881825Z","shell.execute_reply.started":"2025-05-01T08:43:28.640772Z","shell.execute_reply":"2025-05-01T08:43:28.880777Z"}},"outputs":[],"execution_count":195},{"cell_type":"code","source":"#If saved the model as a .zip for persistence and later use\n    \nmodel_dir = '/kaggle/working/sketch_rnn_log_kanji/'\ndata_dir = '/kaggle/working'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T08:43:47.444814Z","iopub.execute_input":"2025-05-01T08:43:47.445147Z","iopub.status.idle":"2025-05-01T08:43:47.449214Z","shell.execute_reply.started":"2025-05-01T08:43:47.445122Z","shell.execute_reply":"2025-05-01T08:43:47.448438Z"}},"outputs":[],"execution_count":196},{"cell_type":"code","source":"def load_env_compatible(data_dir, model_dir):\n    \"\"\"Loads environment for inference mode, used in jupyter notebook.\"\"\"\n    model_params = sketch_rnn_model.get_default_hparams()\n    with tf.io.gfile.GFile(os.path.join(model_dir, 'model_config.json'), 'r') as f:\n        data = json.load(f)\n    fix_list = ['conditional', 'is_training', 'use_input_dropout', 'use_output_dropout', 'use_recurrent_dropout']\n    for fix in fix_list:\n        data[fix] = (data[fix] == 1)\n    model_params.parse_json(json.dumps(data))\n    return load_dataset(data_dir, model_params, inference_mode=True)\n\ndef load_model_compatible(model_dir):\n    model_params = sketch_rnn_model.get_default_hparams()\n    with tf.io.gfile.GFile(os.path.join(model_dir, 'model_config.json'), 'r') as f:\n        data = json.load(f)\n    fix_list = ['conditional', 'is_training', 'use_input_dropout', 'use_output_dropout', 'use_recurrent_dropout']\n    for fix in fix_list:\n        data[fix] = (data[fix] == 1)\n    model_params.parse_json(json.dumps(data))\n    \n    model_params.batch_size = 1\n    model_params.input_size = 133  # ðŸ”¥ KEY LINE TO FIX MISMATCH\n    model_params.conditional = True\n    \n    eval_model_params = sketch_rnn_model.copy_hparams(model_params)\n    eval_model_params.use_input_dropout = 0\n    eval_model_params.use_recurrent_dropout = 0\n    eval_model_params.use_output_dropout = 0\n    eval_model_params.is_training = 0\n    eval_model_params.input_size = 133  # ðŸ”¥\n\n    sample_model_params = sketch_rnn_model.copy_hparams(eval_model_params)\n    sample_model_params.max_seq_len = 1\n    sample_model_params.input_size = 133  # ðŸ”¥\n\n    return [model_params, eval_model_params, sample_model_params]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T08:43:48.987916Z","iopub.execute_input":"2025-05-01T08:43:48.988240Z","iopub.status.idle":"2025-05-01T08:43:48.995709Z","shell.execute_reply.started":"2025-05-01T08:43:48.988215Z","shell.execute_reply":"2025-05-01T08:43:48.994847Z"}},"outputs":[],"execution_count":197},{"cell_type":"code","source":"hps_model, eval_hps_model, sample_hps_model = load_model_compatible(model_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T08:43:50.510219Z","iopub.execute_input":"2025-05-01T08:43:50.510513Z","iopub.status.idle":"2025-05-01T08:43:50.515328Z","shell.execute_reply.started":"2025-05-01T08:43:50.510488Z","shell.execute_reply":"2025-05-01T08:43:50.514511Z"}},"outputs":[],"execution_count":198},{"cell_type":"code","source":"[train_set, valid_set, test_set, hps_model, eval_hps_model, sample_hps_model] = load_env_compatible(data_dir, model_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T08:43:51.434741Z","iopub.execute_input":"2025-05-01T08:43:51.435039Z","iopub.status.idle":"2025-05-01T08:43:51.476747Z","shell.execute_reply.started":"2025-05-01T08:43:51.435017Z","shell.execute_reply":"2025-05-01T08:43:51.476017Z"}},"outputs":[{"name":"stdout","text":"total images <= max_seq_len is 173\ntotal images <= max_seq_len is 22\ntotal images <= max_seq_len is 22\n","output_type":"stream"}],"execution_count":199},{"cell_type":"code","source":"import tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\n\ndef reset_graph():\n    \"\"\"Resets the default graph (compatible with TF1-style code).\"\"\"\n    tf.reset_default_graph()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T08:43:53.551198Z","iopub.execute_input":"2025-05-01T08:43:53.551591Z","iopub.status.idle":"2025-05-01T08:43:53.556205Z","shell.execute_reply.started":"2025-05-01T08:43:53.551551Z","shell.execute_reply":"2025-05-01T08:43:53.555239Z"}},"outputs":[],"execution_count":200},{"cell_type":"code","source":"# construct the sketch-rnn model here:\nreset_graph()\nmodel = Model(hps_model)\neval_model = Model(eval_hps_model, reuse=True)\nsample_model = Model(sample_hps_model, reuse=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T08:43:54.922355Z","iopub.execute_input":"2025-05-01T08:43:54.922701Z","iopub.status.idle":"2025-05-01T08:43:57.446993Z","shell.execute_reply.started":"2025-05-01T08:43:54.922671Z","shell.execute_reply":"2025-05-01T08:43:57.446044Z"}},"outputs":[],"execution_count":201},{"cell_type":"code","source":"import os\nprint(os.listdir(\"/kaggle/working/sketch_rnn_log_kanji\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T08:43:59.741497Z","iopub.execute_input":"2025-05-01T08:43:59.741841Z","iopub.status.idle":"2025-05-01T08:43:59.746717Z","shell.execute_reply.started":"2025-05-01T08:43:59.741813Z","shell.execute_reply":"2025-05-01T08:43:59.745850Z"}},"outputs":[{"name":"stdout","text":"['vector-1000.data-00000-of-00001', 'vector-1000.meta', 'checkpoint', 'vector-1000.index', 'model_config.json']\n","output_type":"stream"}],"execution_count":202},{"cell_type":"code","source":"def load_checkpoint(sess, checkpoint_path):\n    \"\"\"Loads a checkpoint given the full path to the checkpoint file (no extension).\"\"\"\n    saver = tf.train.Saver()\n    saver.restore(sess, checkpoint_path)\n    print(f\"âœ… Restored checkpoint from {checkpoint_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T08:44:02.517340Z","iopub.execute_input":"2025-05-01T08:44:02.517706Z","iopub.status.idle":"2025-05-01T08:44:02.521810Z","shell.execute_reply.started":"2025-05-01T08:44:02.517676Z","shell.execute_reply":"2025-05-01T08:44:02.520939Z"}},"outputs":[],"execution_count":203},{"cell_type":"code","source":"sess = tf.InteractiveSession()\nsess.run(tf.global_variables_initializer())\n\nload_checkpoint(sess, \"/kaggle/working/sketch_rnn_log_kanji/vector-1000\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T08:44:50.629036Z","iopub.execute_input":"2025-05-01T08:44:50.629360Z","iopub.status.idle":"2025-05-01T08:44:50.868882Z","shell.execute_reply.started":"2025-05-01T08:44:50.629332Z","shell.execute_reply":"2025-05-01T08:44:50.868086Z"}},"outputs":[{"name":"stdout","text":"âœ… Restored checkpoint from /kaggle/working/sketch_rnn_log_kanji/vector-1000\n","output_type":"stream"}],"execution_count":206},{"cell_type":"code","source":"#We define two convenience functions to encode a stroke into a latent vector, and decode from latent vector to stroke.\ndef encode(input_strokes):\n    strokes = to_big_strokes(input_strokes).tolist()\n    strokes.insert(0, [0, 0, 1, 0, 0])  # Add start token\n\n    seq = np.array(strokes)\n    max_seq_len = eval_model.hps.max_seq_len\n    actual_len = min(len(seq), max_seq_len)\n\n    # ðŸ›  Force shape to be (1, max_seq_len, 5)\n    strokes_np = np.zeros((1, max_seq_len+1, 5), dtype=float)\n    strokes_np[0, :actual_len, :] = seq[:actual_len]\n\n    draw_strokes(to_normal_strokes(seq[:actual_len]))  # optional preview\n\n    print(\"Input shape:\", strokes_np.shape)\n    print(\"Sequence length fed:\", actual_len)\n    print(\"Expected max_seq_len:\", max_seq_len)\n\n    return sess.run(\n        eval_model.batch_z,\n        feed_dict={\n            eval_model.input_data: strokes_np,\n            eval_model.sequence_lengths: [actual_len]\n        }\n    )[0]\n\n\n\ndef decode(z_input=None, draw_mode=True, temperature=0.1, factor=0.1):\n  z = None\n  if z_input is not None:\n    z = [z_input]\n  sample_strokes, m = sample(sess, sample_model, seq_len=eval_model.hps.max_seq_len, temperature=temperature, z=z)\n  strokes = to_normal_strokes(sample_strokes)\n  if draw_mode:\n    draw_strokes(strokes, factor)\n  return strokes\n     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T08:44:53.884461Z","iopub.execute_input":"2025-05-01T08:44:53.884777Z","iopub.status.idle":"2025-05-01T08:44:53.891570Z","shell.execute_reply.started":"2025-05-01T08:44:53.884753Z","shell.execute_reply":"2025-05-01T08:44:53.890700Z"}},"outputs":[],"execution_count":207},{"cell_type":"code","source":"import xml.etree.ElementTree as ET\nimport numpy as np\n\n#fix the issue of invisible or off-canvas elements drawn \n\ndef is_within_canvas(points, xlim=(0, 576), ylim=(0, 384)):\n    for x, y in points:\n        if xlim[0] <= x <= xlim[1] and ylim[0] <= y <= ylim[1]:\n            return True\n    return False\n\n\ndef svg_polyline_to_stroke3(svg_path, min_stroke_len=2.0):   \n    tree = ET.parse(svg_path)\n    root = tree.getroot()\n    ns = {'svg': 'http://www.w3.org/2000/svg'}\n    polylines = root.findall('.//svg:polyline', ns)\n\n    strokes = []\n    abs_x, abs_y = 0, 0  # Current pen position\n\n    for poly in polylines:\n        points_attr = poly.attrib.get(\"points\", \"\")\n        points = [tuple(map(float, pt.strip().split(',')))\n                  for pt in points_attr.strip().split(' ') if pt.strip()]\n        \n        # Reject if all points are same\n        if len(set(points)) < 2:\n            continue\n\n        # Reject if total distance is too short\n        dist = sum(\n            ((points[i][0] - points[i - 1][0]) ** 2 + (points[i][1] - points[i - 1][1]) ** 2) ** 0.5\n            for i in range(1, len(points))\n        )\n        if dist < min_stroke_len:\n            continue\n        if not is_within_canvas(points):\n            continue\n            \n        # Move to stroke start (pen up)\n        first_x, first_y = points[0]\n        dx, dy = first_x - abs_x, first_y - abs_y\n        strokes.append((dx, dy, 1))  # pen up\n        abs_x, abs_y = first_x, first_y\n\n        for i, (x, y) in enumerate(points[1:], start=1):\n            dx = x - abs_x\n            dy = y - abs_y\n            pen = 0 if i < len(points) - 1 else 1  # lift at last point\n            strokes.append((dx, dy, pen))\n            abs_x, abs_y = x, y\n\n    return strokes\n\n\ndef normalize_stroke3(strokes, canvas_size=256):\n    # convert to absolute coordinates\n    abs_points = []\n    x, y = 0, 0\n    for dx, dy, pen in strokes:\n        x += dx\n        y += dy\n        abs_points.append((x, y, pen))\n    abs_points = np.array(abs_points)\n\n    # compute bounds\n    min_x, max_x = np.min(abs_points[:, 0]), np.max(abs_points[:, 0])\n    min_y, max_y = np.min(abs_points[:, 1]), np.max(abs_points[:, 1])\n\n    # center and scale to fit canvas\n    center_x = (max_x + min_x) / 2\n    center_y = (max_y + min_y) / 2\n    scale = max(max_x - min_x, max_y - min_y)\n    if scale == 0:\n        scale = 1.0\n\n    # fit into canvas\n    target_scale = canvas_size * 0.8  # leave margins\n    normalized_points = []\n    for x, y, pen in abs_points:\n        nx = ((x - center_x) / scale) * target_scale\n        ny = ((y - center_y) / scale) * target_scale\n        normalized_points.append((nx, ny, pen))\n\n    # convert back to stroke-3 deltas\n    stroke3_normalized = []\n    prev_x, prev_y = 0, 0\n    for x, y, pen in normalized_points:\n        dx = x - prev_x\n        dy = y - prev_y\n        stroke3_normalized.append((dx, dy, pen))\n        prev_x, prev_y = x, y\n\n    return np.array(stroke3_normalized, dtype=float)\n\n#if stroke too long\ndef truncate_stroke(stroke, max_len):\n    \"\"\"\n    Truncates a stroke sequence to fit within max_len.\n    Useful when preparing input for Sketch-RNN models.\n    \"\"\"\n    if len(stroke) > max_len:\n        print(f\"âš ï¸ Stroke too long ({len(stroke)}), truncating to {max_len} points.\")\n        return stroke[:max_len]\n    return stroke\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# batch convert eng.svg to stroke 3 \nimport os\nfrom tqdm import tqdm\nimport numpy as np\n\ninput_dir = \"/kaggle/input/emma-sketches/emma_drawings\"\noutput_npz_path = \"/kaggle/working/emma.npz\"\nmax_len = 250  # set max stroke length for model compatibility\n\nall_strokes = []\n\nsvg_files = sorted([f for f in os.listdir(input_dir) if f.endswith(\".svg\")])\nprint(f\"Found {len(svg_files)} SVG files.\")\n\nfor fname in tqdm(svg_files, desc=\"Processing SVGs\"):\n    svg_path = os.path.join(input_dir, fname)\n    try:\n        stroke3 = svg_polyline_to_stroke3(svg_path)\n        if len(stroke3) == 0:\n            continue  # skip empty strokes\n        stroke3 = normalize_stroke3(stroke3, canvas_size=256)\n        stroke3 = truncate_stroke(stroke3, max_len=max_len)\n        all_strokes.append(stroke3)\n    except Exception as e:\n        print(f\"âŒ Failed to process {fname}: {e}\")\n\nprint(f\"âœ… Collected {len(all_strokes)} stroke-3 samples.\")\n\n# Save all strokes to a single .npz file\nnp.savez_compressed(output_npz_path, *all_strokes)\nprint(f\"âœ… Saved to: {output_npz_path}\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#split into train, valid, test \nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# Load from flat .npz (arr_0, arr_1, ...)\nraw = np.load('/kaggle/working/emma.npz', allow_pickle=True)\nall_strokes = [raw[k] for k in raw.files]\n\n# Split\ntrain, rest = train_test_split(all_strokes, test_size=0.2, random_state=42)\nvalid, test = train_test_split(rest, test_size=0.5, random_state=42)\n\n# Save English-only Sketch-RNN dataset\nnp.savez_compressed('/kaggle/working/emma_skt.npz',\n    train=np.array(train, dtype=object),\n    valid=np.array(valid, dtype=object),\n    test=np.array(test, dtype=object)\n)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"english = np.load('/kaggle/working/emma_skt.npz', allow_pickle=True)\nprint(\"Train:\", len(english['train']))\nprint(\"Valid:\", len(english['valid']))\nprint(\"Test:\", len(english['test']))\n\n# Optional: visualize one\ndraw_strokes(english['train'][20])\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#convert scg made with p5.js to stroke-3 \nimport numpy as np\n\nstroke3 = svg_polyline_to_stroke3(\"/kaggle/input/handwritten-english-words/eng/0344.svg\")\nstroke3 = np.array(stroke3, dtype=float)  # required by get_bounds and draw_strokes\nstroke3 = normalize_stroke3(stroke3, canvas_size=52)\n\n# max_len = eval_model.hps.max_seq_len\n# stroke3= truncate_stroke(stroke3, max_len)\n\ndraw_strokes(stroke3)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#comvert moon drawings to stroke-3 \nimport os\nimport numpy as np\nfrom tqdm import tqdm  # nice progress bar\n\ndef tsv_xy_to_stroke3(data):\n    stroke3 = []\n    prev_x, prev_y = data[0]\n    for i in range(1, len(data)):\n        x, y = data[i]\n        dx = x - prev_x\n        dy = y - prev_y\n        pen = 0 if i < len(data) - 1 else 1\n        stroke3.append((dx, dy, pen))\n        prev_x, prev_y = x, y\n    return np.array(stroke3, dtype=float)\n\ndef normalize_stroke3(strokes, canvas_size=256):\n    abs_points = []\n    x, y = 0, 0\n    for dx, dy, pen in strokes:\n        x += dx\n        y += dy\n        abs_points.append((x, y, pen))\n    abs_points = np.array(abs_points)\n\n    min_x, max_x = np.min(abs_points[:, 0]), np.max(abs_points[:, 0])\n    min_y, max_y = np.min(abs_points[:, 1]), np.max(abs_points[:, 1])\n    center_x = (max_x + min_x) / 2\n    center_y = (max_y + min_y) / 2\n    scale = max(max_x - min_x, max_y - min_y)\n    if scale == 0: scale = 1.0\n    target = canvas_size * 0.8\n\n    normalized_points = []\n    for x, y, pen in abs_points:\n        nx = ((x - center_x) / scale) * target\n        ny = ((y - center_y) / scale) * target\n        normalized_points.append((nx, ny, pen))\n\n    stroke3_normalized = []\n    prev_x, prev_y = 0, 0\n    for x, y, pen in normalized_points:\n        dx = x - prev_x\n        dy = y - prev_y\n        stroke3_normalized.append((dx, dy, pen))\n        prev_x, prev_y = x, y\n\n    return np.array(stroke3_normalized, dtype=float)\n\ndef batch_process_tsv(input_dir, output_dir, normalize=True, canvas_size=256):\n    os.makedirs(output_dir, exist_ok=True)\n    files = [f for f in os.listdir(input_dir) if f.endswith('.tsv')]\n\n    for fname in tqdm(files, desc=\"Processing TSVs\"):\n        in_path = os.path.join(input_dir, fname)\n        out_path = os.path.join(output_dir, fname.replace('.tsv', '.npy'))\n\n        try:\n            data = np.loadtxt(in_path, delimiter='\\t')\n            stroke3 = tsv_xy_to_stroke3(data)\n            if normalize:\n                stroke3 = normalize_stroke3(stroke3, canvas_size=canvas_size)\n            np.save(out_path, stroke3)\n        except Exception as e:\n            print(f\"Failed to process {fname}: {e}\")\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_process_tsv(\n    input_dir=\"/kaggle/input/moondrawings-dataset/downloads\",\n    output_dir=\"/kaggle/working/moon_stroke3\",\n    normalize=True,\n    canvas_size=256\n)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom tqdm import tqdm\nimport shutil\n\n# Define paths\ninput_dir = \"/kaggle/working/moon_stroke3\"\noutput_npz_path = \"/kaggle/working/moon_dataset.npz\"\n\n# List all .npy files\nnpy_files = sorted(f for f in os.listdir(input_dir) if f.endswith('.npy'))\n\n# Load and collect all stroke3 arrays\nall_strokes = []\nfor fname in tqdm(npy_files, desc=\"Loading strokes\"):\n    path = os.path.join(input_dir, fname)\n    stroke = np.load(path)\n    all_strokes.append(stroke)\n\n# Save as compressed .npz\nnp.savez_compressed(output_npz_path, *all_strokes)\nprint(f\"Saved {len(all_strokes)} strokes to {output_npz_path}\")\n\n# Delete the folder to free space and prevent UI lag\nshutil.rmtree(input_dir)\nprint(f\"Deleted folder: {input_dir}\")\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#load sketches from quickDraw cat dataset (npz file) \nimport numpy as np\n\n# Load the dataset\ncat_data = np.load('/kaggle/input/face-dataset/sketchrnn_face.npz', allow_pickle=True, encoding='latin1')\n    #QuickDraw.npz file contains pickled Python objects\n    #NumPy is trying to unpickle them using the default ASCII encoding, which fails. \n    #To fix this, specify encoding='latin1'\n\n# Sample a stroke from training set\ncat_stroke = cat_data['train'][38]  # or use random.choice(cat_data['train'])\n\n# Optional: visualize it\ndraw_strokes(cat_stroke)\n\n# Convert to latent vector using your encode function\nz_cat = encode(cat_stroke)\n\n# Generate a new sketch conditioned on the cat\n_ = decode(z_input=z_cat)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# get a sample drawing from the test set, and render it to .svg\nstroke = train_set.random_sample()\ndraw_strokes(stroke)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T08:48:54.591629Z","iopub.execute_input":"2025-05-01T08:48:54.591914Z","iopub.status.idle":"2025-05-01T08:48:54.600181Z","shell.execute_reply.started":"2025-05-01T08:48:54.591892Z","shell.execute_reply":"2025-05-01T08:48:54.599458Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.SVG object>","image/svg+xml":"<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" baseProfile=\"full\" height=\"115.60401439666748\" version=\"1.1\" width=\"104.76037621498108\"><defs/><rect fill=\"white\" height=\"115.60401439666748\" width=\"104.76037621498108\" x=\"0\" y=\"0\"/><path d=\"M52.380186691880226,57.802007645368576 m7.319456338882446,-31.71764373779297 m-2.7109095454216003,-0.5421819537878036 l-2.1687278151512146,-0.5421819537878036 -2.7109095454216003,0.0 l-2.7109095454216003,0.0 -2.7109095454216003,0.0 l-3.25309157371521,0.0 -2.1687278151512146,0.5421819537878036 l-2.7109095454216003,0.0 -2.7109095454216003,0.5421819537878036 l-3.7952736020088196,1.0843639075756073 -2.7109095454216003,2.1687278151512146 l-2.7109095454216003,1.626545786857605 -1.0843639075756073,3.25309157371521 l-0.5421819537878036,2.1687278151512146 0.5421819537878036,3.25309157371521 l0.5421819537878036,2.1687278151512146 1.0843639075756073,2.1687278151512146 l2.1687278151512146,2.7109095454216003 1.626545786857605,1.626545786857605 l2.1687278151512146,1.626545786857605 2.1687278151512146,2.1687278151512146 l3.25309157371521,2.1687278151512146 2.1687278151512146,0.5421819537878036 l2.1687278151512146,1.0843639075756073 3.25309157371521,1.0843639075756073 l2.1687278151512146,0.5421819537878036 4.337455630302429,0.5421819537878036 l2.7109095454216003,0.5421819537878036 3.7952736020088196,-0.5421819537878036 l3.7952736020088196,-0.5421819537878036 3.25309157371521,-0.5421819537878036 l2.1687278151512146,-0.5421819537878036 3.25309157371521,-1.0843639075756073 l2.1687278151512146,-1.0843639075756073 3.7952736020088196,-2.7109095454216003 l1.0843639075756073,-2.1687278151512146 1.0843639075756073,-2.7109095454216003 l0.0,-3.25309157371521 -0.5421819537878036,-2.7109095454216003 l-1.0843639075756073,-2.7109095454216003 -1.626545786857605,-2.7109095454216003 l-2.1687278151512146,-2.1687278151512146 -2.1687278151512146,-1.0843639075756073 l-2.7109095454216003,-1.626545786857605 -2.1687278151512146,-1.0843639075756073 l-2.1687278151512146,-0.5421819537878036 -2.7109095454216003,0.0 m-36.86837196350098,3.7952736020088196 m-0.5421819537878036,3.25309157371521 l0.0,4.337455630302429 0.0,3.25309157371521 l0.0,2.7109095454216003 0.5421819537878036,3.25309157371521 l1.0843639075756073,2.7109095454216003 2.1687278151512146,2.7109095454216003 l1.626545786857605,2.1687278151512146 2.1687278151512146,2.7109095454216003 l1.626545786857605,1.626545786857605 2.1687278151512146,2.1687278151512146 l2.7109095454216003,1.626545786857605 2.1687278151512146,1.0843639075756073 l2.7109095454216003,1.0843639075756073 2.7109095454216003,0.5421819537878036 l2.7109095454216003,0.0 3.25309157371521,0.0 l2.7109095454216003,-0.5421819537878036 3.7952736020088196,-1.0843639075756073 l2.1687278151512146,-0.5421819537878036 2.1687278151512146,-1.0843639075756073 l2.7109095454216003,-1.626545786857605 2.1687278151512146,-1.626545786857605 l1.626545786857605,-1.626545786857605 1.626545786857605,-2.7109095454216003 l1.0843639075756073,-2.1687278151512146 m-28.735642433166504,11.928002834320068 m-0.5421819537878036,2.7109095454216003 l-0.5421819537878036,3.7952736020088196 0.0,3.25309157371521 l-0.5421819537878036,3.7952736020088196 -0.5421819537878036,2.7109095454216003 l-0.5421819537878036,2.1687278151512146 m4.337455630302429,-17.349822521209717 m-0.5421819537878036,2.1687278151512146 l0.0,3.7952736020088196 -0.5421819537878036,2.7109095454216003 l0.0,2.7109095454216003 -0.5421819537878036,2.1687278151512146 l-0.5421819537878036,2.1687278151512146 m-1.0843639075756073,-1.0843639075756073 m0.5421819537878036,2.1687278151512146 m-1.0843639075756073,-4.879637360572815 m-2.1687278151512146,-1.0843639075756073 l-2.7109095454216003,0.0 -2.1687278151512146,0.5421819537878036 l-1.0843639075756073,2.1687278151512146 0.0,2.7109095454216003 l2.1687278151512146,2.1687278151512146 2.1687278151512146,1.626545786857605 l2.1687278151512146,0.5421819537878036 2.7109095454216003,1.0843639075756073 l2.1687278151512146,1.0843639075756073 3.25309157371521,0.0 l2.7109095454216003,0.0 2.1687278151512146,-1.0843639075756073 l1.0843639075756073,-2.1687278151512146 -0.5421819537878036,-2.1687278151512146 l-1.626545786857605,-1.626545786857605 -1.626545786857605,-1.626545786857605 l-2.1687278151512146,-0.5421819537878036 m-12.470184564590454,-49.880738258361816 m2.1687278151512146,0.5421819537878036 l2.1687278151512146,-0.5421819537878036 4.337455630302429,0.0 l2.7109095454216003,0.5421819537878036 2.1687278151512146,0.5421819537878036 l3.25309157371521,0.5421819537878036 2.1687278151512146,1.0843639075756073 l2.7109095454216003,1.0843639075756073 2.1687278151512146,2.1687278151512146 l2.1687278151512146,1.626545786857605 1.626545786857605,2.1687278151512146 l1.626545786857605,3.7952736020088196 0.5421819537878036,2.1687278151512146 l0.5421819537878036,2.1687278151512146 m-18.976367712020874,-11.928002834320068 m1.0843639075756073,3.25309157371521 l3.25309157371521,0.5421819537878036 1.626545786857605,-1.626545786857605 l-2.1687278151512146,-1.0843639075756073 -2.1687278151512146,1.0843639075756073 l2.1687278151512146,2.7109095454216003 2.7109095454216003,0.5421819537878036 l2.1687278151512146,-0.5421819537878036 -3.7952736020088196,-1.626545786857605 l-2.1687278151512146,-0.5421819537878036 -2.7109095454216003,0.5421819537878036 l-0.5421819537878036,2.7109095454216003 \" fill=\"none\" stroke=\"black\" stroke-width=\"1\"/></svg>"},"metadata":{}}],"execution_count":296},{"cell_type":"code","source":"#encode the sample stroke into latent vector \nz = encode(stroke)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T08:48:55.056075Z","iopub.execute_input":"2025-05-01T08:48:55.056386Z","iopub.status.idle":"2025-05-01T08:48:55.163084Z","shell.execute_reply.started":"2025-05-01T08:48:55.056360Z","shell.execute_reply":"2025-05-01T08:48:55.162175Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.SVG object>","image/svg+xml":"<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" baseProfile=\"full\" height=\"115.60401439666748\" version=\"1.1\" width=\"104.76037621498108\"><defs/><rect fill=\"white\" height=\"115.60401439666748\" width=\"104.76037621498108\" x=\"0\" y=\"0\"/><path d=\"M52.380186691880226,57.802007645368576 m0.0,0.0 l7.319456338882446,-31.71764373779297 m-2.7109095454216003,-0.5421819537878036 l-2.1687278151512146,-0.5421819537878036 -2.7109095454216003,0.0 l-2.7109095454216003,0.0 -2.7109095454216003,0.0 l-3.25309157371521,0.0 -2.1687278151512146,0.5421819537878036 l-2.7109095454216003,0.0 -2.7109095454216003,0.5421819537878036 l-3.7952736020088196,1.0843639075756073 -2.7109095454216003,2.1687278151512146 l-2.7109095454216003,1.626545786857605 -1.0843639075756073,3.25309157371521 l-0.5421819537878036,2.1687278151512146 0.5421819537878036,3.25309157371521 l0.5421819537878036,2.1687278151512146 1.0843639075756073,2.1687278151512146 l2.1687278151512146,2.7109095454216003 1.626545786857605,1.626545786857605 l2.1687278151512146,1.626545786857605 2.1687278151512146,2.1687278151512146 l3.25309157371521,2.1687278151512146 2.1687278151512146,0.5421819537878036 l2.1687278151512146,1.0843639075756073 3.25309157371521,1.0843639075756073 l2.1687278151512146,0.5421819537878036 4.337455630302429,0.5421819537878036 l2.7109095454216003,0.5421819537878036 3.7952736020088196,-0.5421819537878036 l3.7952736020088196,-0.5421819537878036 3.25309157371521,-0.5421819537878036 l2.1687278151512146,-0.5421819537878036 3.25309157371521,-1.0843639075756073 l2.1687278151512146,-1.0843639075756073 3.7952736020088196,-2.7109095454216003 l1.0843639075756073,-2.1687278151512146 1.0843639075756073,-2.7109095454216003 l0.0,-3.25309157371521 -0.5421819537878036,-2.7109095454216003 l-1.0843639075756073,-2.7109095454216003 -1.626545786857605,-2.7109095454216003 l-2.1687278151512146,-2.1687278151512146 -2.1687278151512146,-1.0843639075756073 l-2.7109095454216003,-1.626545786857605 -2.1687278151512146,-1.0843639075756073 l-2.1687278151512146,-0.5421819537878036 -2.7109095454216003,0.0 m-36.86837196350098,3.7952736020088196 m-0.5421819537878036,3.25309157371521 l0.0,4.337455630302429 0.0,3.25309157371521 l0.0,2.7109095454216003 0.5421819537878036,3.25309157371521 l1.0843639075756073,2.7109095454216003 2.1687278151512146,2.7109095454216003 l1.626545786857605,2.1687278151512146 2.1687278151512146,2.7109095454216003 l1.626545786857605,1.626545786857605 2.1687278151512146,2.1687278151512146 l2.7109095454216003,1.626545786857605 2.1687278151512146,1.0843639075756073 l2.7109095454216003,1.0843639075756073 2.7109095454216003,0.5421819537878036 l2.7109095454216003,0.0 3.25309157371521,0.0 l2.7109095454216003,-0.5421819537878036 3.7952736020088196,-1.0843639075756073 l2.1687278151512146,-0.5421819537878036 2.1687278151512146,-1.0843639075756073 l2.7109095454216003,-1.626545786857605 2.1687278151512146,-1.626545786857605 l1.626545786857605,-1.626545786857605 1.626545786857605,-2.7109095454216003 l1.0843639075756073,-2.1687278151512146 m-28.735642433166504,11.928002834320068 m-0.5421819537878036,2.7109095454216003 l-0.5421819537878036,3.7952736020088196 0.0,3.25309157371521 l-0.5421819537878036,3.7952736020088196 -0.5421819537878036,2.7109095454216003 l-0.5421819537878036,2.1687278151512146 m4.337455630302429,-17.349822521209717 m-0.5421819537878036,2.1687278151512146 l0.0,3.7952736020088196 -0.5421819537878036,2.7109095454216003 l0.0,2.7109095454216003 -0.5421819537878036,2.1687278151512146 l-0.5421819537878036,2.1687278151512146 m-1.0843639075756073,-1.0843639075756073 m0.5421819537878036,2.1687278151512146 m-1.0843639075756073,-4.879637360572815 m-2.1687278151512146,-1.0843639075756073 l-2.7109095454216003,0.0 -2.1687278151512146,0.5421819537878036 l-1.0843639075756073,2.1687278151512146 0.0,2.7109095454216003 l2.1687278151512146,2.1687278151512146 2.1687278151512146,1.626545786857605 l2.1687278151512146,0.5421819537878036 2.7109095454216003,1.0843639075756073 l2.1687278151512146,1.0843639075756073 3.25309157371521,0.0 l2.7109095454216003,0.0 2.1687278151512146,-1.0843639075756073 l1.0843639075756073,-2.1687278151512146 -0.5421819537878036,-2.1687278151512146 l-1.626545786857605,-1.626545786857605 -1.626545786857605,-1.626545786857605 l-2.1687278151512146,-0.5421819537878036 m-12.470184564590454,-49.880738258361816 m2.1687278151512146,0.5421819537878036 l2.1687278151512146,-0.5421819537878036 4.337455630302429,0.0 l2.7109095454216003,0.5421819537878036 2.1687278151512146,0.5421819537878036 l3.25309157371521,0.5421819537878036 2.1687278151512146,1.0843639075756073 l2.7109095454216003,1.0843639075756073 2.1687278151512146,2.1687278151512146 l2.1687278151512146,1.626545786857605 1.626545786857605,2.1687278151512146 l1.626545786857605,3.7952736020088196 0.5421819537878036,2.1687278151512146 l0.5421819537878036,2.1687278151512146 m-18.976367712020874,-11.928002834320068 m1.0843639075756073,3.25309157371521 l3.25309157371521,0.5421819537878036 1.626545786857605,-1.626545786857605 l-2.1687278151512146,-1.0843639075756073 -2.1687278151512146,1.0843639075756073 l2.1687278151512146,2.7109095454216003 2.7109095454216003,0.5421819537878036 l2.1687278151512146,-0.5421819537878036 -3.7952736020088196,-1.626545786857605 l-2.1687278151512146,-0.5421819537878036 -2.7109095454216003,0.5421819537878036 l-0.5421819537878036,2.7109095454216003 \" fill=\"none\" stroke=\"black\" stroke-width=\"1\"/></svg>"},"metadata":{}},{"name":"stdout","text":"Input shape: (1, 251, 5)\nSequence length fed: 250\nExpected max_seq_len: 250\n","output_type":"stream"}],"execution_count":297},{"cell_type":"code","source":"z_0 = decode(z, temperature=0.6) # convert z back to drawing at temperature x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T08:49:28.845315Z","iopub.execute_input":"2025-05-01T08:49:28.845649Z","iopub.status.idle":"2025-05-01T08:49:29.246207Z","shell.execute_reply.started":"2025-05-01T08:49:28.845619Z","shell.execute_reply":"2025-05-01T08:49:29.245338Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.SVG object>","image/svg+xml":"<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" baseProfile=\"full\" height=\"182.18051415402442\" version=\"1.1\" width=\"152.98021411988884\"><defs/><rect fill=\"white\" height=\"182.18051415402442\" width=\"152.98021411988884\" x=\"0\" y=\"0\"/><path d=\"M79.12648558616638,110.22760508581996 m14.983296394348145,-64.46108341217041 m-5.297486186027527,0.19206926226615906 l-4.671126902103424,-0.7068535685539246 -6.69256329536438,0.11997811496257782 l-4.722035825252533,1.1173899471759796 -5.650924444198608,0.3585120663046837 l-4.360329210758209,1.3253304362297058 -4.257042407989502,1.6073553264141083 l-6.108596324920654,3.288149833679199 -4.086238443851471,2.037828117609024 l-5.296509265899658,3.4358811378479004 -3.355976641178131,2.688388228416443 l-3.623872995376587,4.423880875110626 -2.416483610868454,4.350376129150391 l-0.5515998229384422,4.059214890003204 0.32900743186473846,5.058307647705078 l1.9520293176174164,5.417943000793457 1.269340068101883,4.540480971336365 l1.7582860589027405,4.914669990539551 1.902742087841034,3.753846287727356 l1.8959741294384003,3.3949512243270874 2.437478005886078,4.040690064430237 l1.8538779020309448,2.853143811225891 2.7089717984199524,4.17242169380188 l3.386838138103485,4.491849839687347 2.6610365509986877,2.981521487236023 l2.8188440203666687,3.7244370579719543 3.669329583644867,4.149504899978638 l3.413388431072235,2.9147934913635254 3.4267744421958923,2.051457464694977 l5.1777344942092896,2.1449102461338043 4.026501178741455,0.9448206424713135 l4.7753095626831055,1.4904315769672394 4.0208956599235535,0.7603704929351807 l5.097739100456238,0.19268987700343132 4.249269068241119,-0.4161781445145607 l4.332326054573059,-0.6524500995874405 4.78742778301239,-2.347254455089569 l5.0611162185668945,-3.498009741306305 2.937626838684082,-2.8984245657920837 l3.294370472431183,-4.687409996986389 2.2614046931266785,-5.877924561500549 l1.9519034028053284,-6.452508568763733 0.6877210736274719,-5.509668588638306 l-0.6608650088310242,-6.911653280258179 -1.821448802947998,-7.161219120025635 l-1.6726391017436981,-4.532806575298309 -3.2151198387145996,-5.2762556076049805 l-2.7170488238334656,-4.173762798309326 -4.249617159366608,-3.6551421880722046 l-3.20853590965271,-3.5518577694892883 -3.9850088953971863,-2.694418430328369 l-5.364437699317932,-1.606936901807785 -5.911557674407959,-0.3592108190059662 l-7.677247524261475,1.1753929406404495 -4.691621363162994,5.019315481185913 m-13.014476299285889,18.57831835746765 m3.9615589380264282,-4.126065373420715 l-2.1845300495624542,-4.203801155090332 -2.897917628288269,-3.3183884620666504 l-3.486143946647644,-3.218201994895935 -1.5534916520118713,-3.566265106201172 l-2.161300629377365,-4.157260060310364 -2.1385471522808075,-3.600042164325714 l-3.180437982082367,-3.331318497657776 -2.957574427127838,-2.7802449464797974 m4.521053433418274,3.575173020362854 m0.5788757279515266,4.318232834339142 l-0.4408416152000427,6.091910004615784 0.6233888119459152,5.425732135772705 l1.3828101754188538,6.37370228767395 3.0783379077911377,3.629262149333954 l3.177027404308319,1.9883270561695099 6.513680815696716,2.787468731403351 l3.474205434322357,1.425897628068924 3.108135759830475,-4.045519530773163 l-0.9175967425107956,-6.116184592247009 -2.4082976579666138,-5.024015307426453 l-2.1485234797000885,-4.530651271343231 -1.9515691697597504,-4.124200642108917 l-2.264733165502548,-4.035327136516571 m-37.702205181121826,-9.985640048980713 m4.163967072963715,4.614550471305847 l2.5552278757095337,3.971068263053894 2.848879396915436,3.2183218002319336 l2.1213969588279724,4.747286140918732 4.718641638755798,4.529120028018951 l6.132789850234985,5.014809966087341 3.3892807364463806,3.4887567162513733 l3.9403951168060303,4.392317831516266 3.580097258090973,3.902442753314972 l3.9990463852882385,4.3867820501327515 3.2231733202934265,2.9307061433792114 l3.5612952709198,2.197451740503311 3.528638482093811,2.170800119638443 l6.911680698394775,2.4469587206840515 6.111735105514526,1.5234680473804474 l6.077472567558289,0.37999246269464493 3.9573508501052856,0.9040233492851257 l3.730957508087158,-3.400529623031616 3.1378936767578125,-4.248768389225006 l3.4487873315811157,-5.611801743507385 3.40952605009079,-6.76391065120697 l1.0990112274885178,-6.405853629112244 1.3781268894672394,-6.45025372505188 l1.8155230581760406,-6.877206563949585 0.6529878824949265,-5.293859839439392 l0.5845244228839874,-6.767104864120483 -0.4694672301411629,-4.828398525714874 l-0.926537811756134,-5.896227955818176 -2.3302946984767914,-3.9223814010620117 l-3.0671724677085876,-3.5507288575172424 -3.4191033244132996,-3.844207525253296 l-4.1901373863220215,-3.0513301491737366 m-43.100600242614746,-8.926071524620056 m-1.6192936897277832,4.677686393260956 l-1.5360724925994873,6.719115972518921 -1.432962417602539,7.70663857460022 l-0.7852746546268463,5.793164968490601 -0.03543531056493521,5.0381797552108765 l0.9681770950555801,5.501160025596619 2.4180206656455994,3.9655357599258423 l2.7334368228912354,3.9886659383773804 1.4861276745796204,4.5388758182525635 l0.4662569612264633,4.623948931694031 1.936817467212677,4.394425451755524 l2.1668504178524017,3.685082793235779 3.3280694484710693,3.2070428133010864 l3.3222848176956177,2.6113900542259216 4.085811972618103,-1.7391453683376312 l4.088231027126312,1.048251911997795 2.9546093940734863,3.4831181168556213 l3.367275297641754,1.9874370098114014 2.950555384159088,3.8681846857070923 l3.860951066017151,1.8350960314273834 4.011957347393036,1.212111935019493 l3.760773539543152,1.1251650750637054 4.232920706272125,4.390024840831757 l2.3142142593860626,3.9354509115219116 3.675522804260254,2.2710123658180237 l3.4916576743125916,1.1392492800951004 3.535182774066925,1.2963664531707764 l3.700934052467346,0.5159201845526695 3.5047483444213867,0.040774871595203876 l3.4527722001075745,3.397991955280304 m-27.54220485687256,-63.462934494018555 m5.086216330528259,3.0150780081748962 l3.081558048725128,4.294974207878113 3.142727017402649,6.1193013191223145 l3.6906051635742188,6.731272339820862 4.370807409286499,7.059527039527893 l1.3937143981456757,5.553252100944519 1.128622591495514,5.963848233222961 l0.7816227525472641,6.201242804527283 0.49714691936969757,5.031072497367859 l0.10647052899003029,4.824917316436768 -0.243344958871603,4.559455215930939 l-0.7002822309732437,4.370554089546204 -2.3324985802173615,4.102948904037476 m-0.09787121787667274,-21.15225315093994 m2.24588081240654,5.302973389625549 l0.8415472507476807,5.733480453491211 1.0089236497879028,4.997401237487793 l1.280646175146103,5.051003694534302 0.9899996221065521,5.064607858657837 l0.004462201031856239,4.843343496322632 0.23682599887251854,5.505732297897339 l-0.1559806987643242,4.932403266429901 -0.25394484400749207,4.752759039402008 l-1.1132626235485077,4.636812210083008 -4.5942723751068115,1.8898172676563263 l-3.0156588554382324,-1.902143806219101 -2.688175141811371,-2.0928581058979034 l-1.9076913595199585,-3.1066417694091797 \" fill=\"none\" stroke=\"black\" stroke-width=\"1\"/></svg>"},"metadata":{}}],"execution_count":309},{"cell_type":"code","source":"#Create generated grid at various temperatures from 0.1 to 1.0\n\nstroke_list = []\nfor i in range(10):\n  stroke_list.append([decode(z_1, draw_mode=False, temperature=0.1*i+0.1), [0, i]])\nstroke_grid = make_grid_svg(stroke_list)\ndraw_strokes(stroke_grid)","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-05-01T08:45:48.225100Z","iopub.execute_input":"2025-05-01T08:45:48.225442Z","iopub.status.idle":"2025-05-01T08:45:48.244501Z","shell.execute_reply.started":"2025-05-01T08:45:48.225414Z","shell.execute_reply":"2025-05-01T08:45:48.243485Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-225-1d8dc636449b>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstroke_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mstroke_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdraw_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mstroke_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_grid_svg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstroke_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdraw_strokes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstroke_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'z_1' is not defined"],"ename":"NameError","evalue":"name 'z_1' is not defined","output_type":"error"}],"execution_count":225},{"cell_type":"code","source":"# Get two random strokes from the test set\nstroke_0 = test_set.random_sample()\nstroke_1 = test_set.random_sample()\n\n# Encode them into latent vectors\nz0 = encode(stroke_0)\nz1 = encode(stroke_1)\n\n# Interpolate between them\nN = 10\nz_list = [slerp(z0, z1, t) for t in np.linspace(0, 1, N)]\n\n# Decode interpolated latent vectors into stroke sequences\ninterpolations = [decode(z, temperature=1) for z in z_list]\n\n# stroke_grid = make_grid_svg(reconstructions)\n# draw_strokes(stroke_grid)\n\n# Visualize the sequence one by one (X grid)\nfor i, s in enumerate(interpolations):\n    print(f\"Rendering step {i}\")\n    draw_strokes(s)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"z_list = []\nfor t in np.linspace(0, 1, N):\n  z_list.append(slerp(z0, z1, t))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stroke = test_set.random_sample()\nz = encode(stroke)\nz_2 = decode (z, temperature = 0.8)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#interpolate between z1 and z1\n\nz_list = [] # interpolate spherically between z0 and z1\nN = 10\nfor t in np.linspace(0, 1, N):\n  z_list.append(slerp(z_0, z_2, t))\n\n# for every latent vector in z_list, sample a vector image\nreconstructions = []\nfor i in range(N):\n  reconstructions.append([decode(z_list[i], draw_mode=False, temperature = 0.8), [0, i]])\n\nstroke_grid = make_grid_svg(reconstructions)\ndraw_strokes(stroke_grid)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Parameters\nN_rows = 10     # number of interpolation steps (z vectors)\nN_cols = 10     # number of temperatures per row\ntemperatures = np.linspace(0.1, 1.0, N_cols)\n\n# Interpolate between z_1 and z_2\nz_grid = [slerp(z0, z1, t) for t in np.linspace(0, 1, N_rows)]\n\n# Generate the strokes grid\nstroke_grid_data = []\n\nfor row_idx, z in enumerate(z_grid):\n    for col_idx, temp in enumerate(temperatures):\n        sketch = decode(z, draw_mode=False, temperature=temp)\n        stroke_grid_data.append([sketch, [col_idx, row_idx]])\n\n# Render as grid\nstroke_grid = make_grid_svg(stroke_grid_data)\ndraw_strokes(stroke_grid)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stroke = stroke3\nincomplete = stroke[:19]\ndraw_strokes (incomplete)\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def decode_from_partial(partial_stroke, temperature=0.4, max_len=134):\n    # Convert input stroke to big_strokes format\n    stroke_big = [to_big_strokes(partial_stroke, max_len=134)]\n    seq_len = [len(partial_stroke)]\n\n    # Encode partial stroke into final decoder state\n    final_state = sess.run(eval_model.final_state, feed_dict={\n        eval_model.input_data: stroke_big,\n        eval_model.sequence_lengths: seq_len\n    })\n\n    # Start token for generation\n    input_token = np.array([[[0, 0, 1, 0, 0]]])  # (batch=1, len=1, dim=5)\n\n    state = final_state\n    generated = []\n\n    for i in range(max_len):\n        feed = {\n            sample_model.input_data: input_token,\n            sample_model.initial_state: state\n        }\n        output, state = sess.run(\n            [sample_model.output, sample_model.final_state], feed_dict=feed\n        )\n\n        # Sample next point from GMM\n        next_token = sample_next_point(output[0], temperature=temperature)\n\n        generated.append(next_token)\n\n        input_token = np.expand_dims([next_token], 0)  # shape: (1, 1, 5)\n\n        # Optionally stop early if EOS token (third dim == 1)\n        if next_token[2] == 1:\n            break\n\n    # Convert to numpy and return\n    return np.array(generated)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from magenta.models.sketch_rnn.utils import to_big_strokes\n\n# Step 1: Convert incomplete to big strokes with correct padding\nincomplete_big = to_big_strokes(incomplete, max_len=134)  # make sure this is 134\nincomplete_big = np.expand_dims(incomplete_big, 0)  # shape: (1, 134, 5)\n\n# Step 2: Set correct sequence length (only the valid portion)\nseq_len = [len(incomplete)]  # NOT 134! Just the actual number of valid steps\nprint(\"input shape:\", incomplete_big.shape)\nprint(\"seq_len:\", seq_len)\nprint(\"expected input shape:\", eval_model.input_data.shape)\n\n\n# Step 3: Run the encoder or final decoder state\nfinal_state = sess.run(\n    eval_model.final_state,\n    feed_dict={\n        eval_model.input_data: incomplete_big,\n        eval_model.sequence_lengths: seq_len\n    }\n)\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"completed = decode_from_partial(incomplete, temperature=0.4)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#load the Flamingo Model\n#Unconditional (Decoder-Only) Generation\n\nmodel_dir = '/kaggle/working/sketch_rnn_log'\n[hps_model, eval_hps_model, sample_hps_model] = load_model_compatible(model_dir)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# construct the sketch-rnn model here:\nreset_graph()\nmodel = Model(hps_model)\neval_model = Model(eval_hps_model, reuse=True)\nsample_model = Model(sample_hps_model, reuse=True)\n\nsess = tf.InteractiveSession()\nsess.run(tf.global_variables_initializer())\n\n# loads the weights from checkpoint into our model\nload_checkpoint(sess, model_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# randomly unconditionally generate 10 examples\nN = 10\nreconstructions = []\nfor i in range(N):\n  reconstructions.append([decode(temperature=0.5, draw_mode=False), [0, i]])\n     ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stroke_grid = make_grid_svg(reconstructions)\ndraw_strokes(stroke_grid)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}